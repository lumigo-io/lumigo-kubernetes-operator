# Kubernetes Data Enhancer Processor

| Status                   |              |
|--------------------------|--------------|
| Stability                | [alpha]      |
| Supported pipeline types | logs, traces |
| Distributions            | [lumigo-k8s] |

This processor ensures that we send a variety of necessary data to the downstream OpenTelemetry endpoint.

## Enhancements

## Trace data

For trace data, it ensures that the `k8s.namespace.uid`, `k8s.deployment.uid` and `k8s.cronjob.uid` resource attributes are set if the `k8s.namespace.name`, `k8s.deployment.name` and `k8s.cronjob.name` are, respectively.
This addresses a limitation of the [`k8sattributesprocessor`](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/k8sattributesprocessor) that, since it only uses the owner-reference data from the pod, rather than also keeping track of `apps/v1.Deployment` and `apps/v1.CronJob` object, can guess the name of the deployment and the cronjob by parsing the name of the `apps/v1.ReplicaSet` and `batch/v1.Job`, but not find their UIDs.

## Log data

The goal is to ensure we can have smooth and easy correlation of issues (e.g., `v1.Event` objects with `WARNING` type) with the history of the objects they affect (which, in `v1.Event` terms, it's the object referenced to by the `involvedObject` field).
Here we do two things:

1. Add the `rootOwnerReference` field of type `v1.ObjectReference` to `v1.Event` that stream through `k8sdataenricherprocessor` in the format generated by [`k8sobjectsreceiver`](https://github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sobjectsreceiver).
  The `rootOwnerReference` is the [workload object](https://kubernetes.io/docs/concepts/workloads/) at the top of the hierarchy of the event's `involvedObject`.
  For example, if the `involvedObject` points to a `v1.Pod`, and the pod is scheduled by a `v1.ReplicaSet` owned by a `v1.Deployment`, the `rootOwnerReference` is the deployment.
  In cases in which the hierarchy has only one level (e.g., an event affecting a deployment or a standalone pod), the `rootOwnerReference` will be equivalent to the `involvedObject`.
  In Lumigo, the `rootOwnerReference` allows us to group events into issues that make immediate sense for the user: for example, if multiple pods in the same deployment crash, the issue is almost always to be found in its [`v1.PodTemplateSpec`](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-template-v1/#PodTemplateSpec) rather than the single pods.
2. If a specific [`resourceVersion`](https://kubernetes.io/docs/reference/using-api/api-concepts/#efficient-detection-of-changes) of an object is sent downstream through as an `involvedObject` of an `v1.Event`, this processor will try _really hard_ to ensure that that specific `resourceVersion` is sent downstream in its entirety in the same format as used by [`k8sobjectsreceiver`](https://github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sobjectsreceiver).
  In the Lumigo backend, this allows us to bridge the gap between `resourceVersions` and [`generations`](https://stackoverflow.com/a/66092577/6188451) (that is, the "versions" of the workload resources with changes that actually affect which pods are scheduled), and know what was the actual structure of an object affected by an issue.

## Implementation

This processor was hard to come up with for a number of reasons:
1. The Kubernetes API is eventually consistent, yet the Lumigo backend wants strong consistency of data (e.g., all the needed `resourceVersion`s)
2. The `resourceVersion`s of an object are around only for a very limited amount of time (it seems on most Kubernetes clusters around 5 minutes), after which they can get garbage-collected; so, collecting the ones we are not sure we sent downstream is a time-sensitive issue.
3. When the Lumigo operator sees a new namespace being monitored, or the stop of monitoring a namespace, the configuration of the entire `telemetry-proxy` is reloaded; for "normal" processors, that means they get re-instantiated, and whatever caches we would be normally lost.

The solution is to keep a (1) single [Kubernetes watch](https://kubernetes.io/docs/reference/using-api/api-concepts/) stream per type of object we care about, i.e.:
* [`v1/Pod`](https://kubernetes.io/docs/concepts/workloads/pods/)
* [`apps/v1.Deployment`](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)
* [`apps/v1.DaemonSet`](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)
* [`apps/v1.ReplicaSet`](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
* [`apps/v1.StatefulSet`](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)
* [`batch/v1.CronJob`](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/)
* [`batch/v1.Job`](https://kubernetes.io/docs/concepts/workloads/controllers/job/)

The watch is not dependent on the lifecycle of the instances of `k8sdataenricherprocessor` that use it.
It effectively stops only when the `telemetry-proxy` process is shutdown.
This way, the caches are not lost on configuration reloads.

The data we get is stored in LRU caches that use a combination of the object UID and `resourceVersion` as cache keys.
As logs issued by the [`k8sobjectsreceiver`](https://github.com/open-telemetry/opentelemetry-collector-contrib/receiver/k8sobjectsreceiver), we also add those objects and `resouerceVersion`s into the cache, so that we have fewer misses.
When we need a `resourceVersion` we do not have and that we need to stream to the backend, we go and immediately get it from the Kube API, using retries with exponential backoffs to avoid failing when the `resourceVersion` still exists in the system, but is now yet available via the API (which is surprisingly common for events about workload objects being created or modified).

[alpha]: https://github.com/open-telemetry/opentelemetry-collector#alpha
[lumigo-k8s]: https://github.com/lumigo-io/lumigo-kubernetes-operator/telemetryproxy
